# app.py

import streamlit as st
import os
import json
from collections import defaultdict

# âœ… ì•± ì‹¤í–‰ ì „ ì•ˆì „ ì´ˆê¸°í™”
try:
    import openai
    openai.api_key = st.secrets["OPENAI_API_KEY"]
    os.makedirs("data", exist_ok=True)
    os.makedirs("outputs", exist_ok=True)
except Exception as e:
    st.error(f"ì•± ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ âŒ: {e}")
    st.stop()

# âœ… ì™¸ë¶€ ëª¨ë“ˆì€ importë§Œ
from modules import file_handler, embedder, clustering, gpt_analyzer, recommender

# âœ… Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ“„ ë¬¸ì„œ ë¶„ì„ ë° ì¶”ì²œ", layout="wide")
st.title("ğŸ“„ ë¬¸ì„œ ì˜ë¯¸ ë¶„ì„ ë° ì¶”ì²œ í”Œë«í¼")

# âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "doc_texts" not in st.session_state:
    st.session_state.doc_texts = {}
if "doc_vectors" not in st.session_state:
    st.session_state.doc_vectors = {}

# STEP 0: íŒŒì¼ ì—…ë¡œë“œ
uploaded_files = st.file_uploader(
    "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.pdf, .md, .txt)",
    type=["pdf", "md", "txt"],
    accept_multiple_files=True
)

if uploaded_files:
    for file in uploaded_files:
        doc_id = file.name
        try:
            text = file_handler.load_file(file)
            chunks = file_handler.split_chunks(text)
            avg_vector = embedder.process_and_store_embeddings(chunks, doc_id)

            st.session_state.doc_texts[doc_id] = text
            st.session_state.doc_vectors[doc_id] = avg_vector

            st.success(f"âœ… {doc_id} ì²˜ë¦¬ ì™„ë£Œ")
        except Exception as e:
            st.error(f"âŒ {doc_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

# STEP 3~5: ë¶„ì„ ì‹¤í–‰
if st.button("ğŸš€ ì˜ë¯¸ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰"):
    if not st.session_state.doc_vectors:
        st.warning("ğŸ“‚ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        st.subheader("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")

        # í´ëŸ¬ìŠ¤í„°ë§
        doc_ids = list(st.session_state.doc_vectors.keys())
        vectors = list(st.session_state.doc_vectors.values())
        cluster_map = clustering.cluster_embeddings(vectors, doc_ids)
        cluster_map = clustering.merge_small_clusters(cluster_map)

        # í´ëŸ¬ìŠ¤í„°ë³„ ê·¸ë£¹í•‘
        clusters = defaultdict(list)
        for doc_id, cluster_id in cluster_map.items():
            clusters[cluster_id].append(doc_id)

        for cluster_id, doc_list in clusters.items():
            st.markdown(f"### ğŸ“ í´ëŸ¬ìŠ¤í„° {cluster_id}")
            texts = [st.session_state.doc_texts[doc_id] for doc_id in doc_list]

            try:
                result_json = gpt_analyzer.summarize_cluster(texts)
                result = json.loads(result_json)

                st.write(f"ğŸ“Œ ì£¼ì œ: **{result['cluster_topic']}**")
                st.write(f"ğŸ“ ìš”ì•½: {result['cluster_summary']}")
                st.write("ğŸ”‘ í‚¤ì›Œë“œ:", ", ".join([f"`{kw}`" for kw in result["keywords"]]))

            except json.JSONDecodeError:
                st.error("âŒ GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: JSON í˜•ì‹ ì•„ë‹˜")
                st.code(result_json)
            except Exception as e:
                st.error(f"âŒ GPT ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

            st.write("ğŸ“„ ë¬¸ì„œ ëª©ë¡:")
            for doc_id in doc_list:
                st.markdown(f"- {doc_id}")

        # STEP 5: ìœ ì‚¬ ë¬¸ì„œ ì¶”ì²œ
        st.subheader("ğŸ“š ìœ ì‚¬ ë¬¸ì„œ ì¶”ì²œ ì˜ˆì‹œ")

        try:
            target_doc = doc_ids[0]
            target_vec = st.session_state.doc_vectors[target_doc]
            other_vectors = [vec for i, vec in enumerate(vectors) if doc_ids[i] != target_doc]
            other_ids = [doc_ids[i] for i in range(len(doc_ids)) if doc_ids[i] != target_doc]

            top_idxs, _ = recommender.recommend_by_cosine(target_vec, other_vectors)
            top_docs = [other_ids[i] for i in top_idxs]
            pairs = [(doc_id, st.session_state.doc_texts[doc_id]) for doc_id in top_docs]

            explanation_json = recommender.explain_document_similarity(
                st.session_state.doc_texts[target_doc],
                pairs
            )

            explanation = json.loads(explanation_json)
            for rec in explanation["recommendations"]:
                st.markdown(f"ğŸ”— **{rec['document_id']}**: {rec['reason']}")
        except Exception as e:
            st.error(f"âŒ ì¶”ì²œ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")

# STEP 6: ZIP ë‹¤ìš´ë¡œë“œ
zip_path = "outputs/summaries.zip"
if os.path.exists(zip_path):
    with open(zip_path, "rb") as f:
        st.download_button(
            label="ğŸ“¦ ê²°ê³¼ ZIP ë‹¤ìš´ë¡œë“œ",
            data=f,
            file_name="summaries.zip",
            mime="application/zip"
        )

