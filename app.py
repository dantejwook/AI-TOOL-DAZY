# app.py

import streamlit as st
import os
from modules import file_handler, embedder, clustering, gpt_analyzer, recommender
import json

EMBED_PATH = "data/embeddings.json"
META_PATH = "data/metadata.json"

st.set_page_config(page_title="ğŸ“„ ë¬¸ì„œ ë¶„ì„ ë° ì¶”ì²œ", layout="wide")
st.title("ğŸ“„ ë¬¸ì„œ ì˜ë¯¸ ë¶„ì„ ë° ì¶”ì²œ í”Œë«í¼")

if "doc_texts" not in st.session_state:
    st.session_state.doc_texts = {}
if "doc_vectors" not in st.session_state:
    st.session_state.doc_vectors = {}

# STEP 0: íŒŒì¼ ì—…ë¡œë“œ
uploaded_files = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.pdf, .md, .txt)", type=["pdf", "md", "txt"], accept_multiple_files=True)

if uploaded_files:
    for file in uploaded_files:
        doc_id = file.name
        text = file_handler.load_file(file)
        chunks = file_handler.split_chunks(text)
        avg_vector = embedder.process_and_store_embeddings(chunks, doc_id)

        st.session_state.doc_texts[doc_id] = text
        st.session_state.doc_vectors[doc_id] = avg_vector

    st.success(f"{len(uploaded_files)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ âœ…")

# STEP 3~5: ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
if st.button("ğŸš€ ì˜ë¯¸ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰"):
    if not st.session_state.doc_vectors:
        st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        st.subheader("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")

        # í´ëŸ¬ìŠ¤í„°ë§
        doc_ids = list(st.session_state.doc_vectors.keys())
        vectors = list(st.session_state.doc_vectors.values())
        cluster_map = clustering.cluster_embeddings(vectors, doc_ids)
        cluster_map = clustering.merge_small_clusters(cluster_map)

        # í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ê·¸ë£¹
        from collections import defaultdict
        clusters = defaultdict(list)
        for doc_id, cluster_id in cluster_map.items():
            clusters[cluster_id].append(doc_id)

        for cluster_id, doc_list in clusters.items():
            st.markdown(f"### ğŸ“ í´ëŸ¬ìŠ¤í„° {cluster_id}")
            texts = [st.session_state.doc_texts[doc_id] for doc_id in doc_list]
            result_json = gpt_analyzer.summarize_cluster(texts)

            try:
                result = json.loads(result_json)
                st.write(f"ğŸ“Œ ì£¼ì œ: **{result['cluster_topic']}**")
                st.write(f"ğŸ“ ìš”ì•½: {result['cluster_summary']}")
                st.write("ğŸ”‘ í‚¤ì›Œë“œ:", ", ".join([f"`{kw}`" for kw in result["keywords"]]))
            except Exception as e:
                st.error("GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜:", result_json)

            st.write("ğŸ“„ ë¬¸ì„œ ëª©ë¡:")
            for doc_id in doc_list:
                st.markdown(f"- {doc_id}")

        # STEP 5: ì¶”ì²œ ì˜ˆì‹œ
        st.subheader("ğŸ“š ìœ ì‚¬ ë¬¸ì„œ ì¶”ì²œ ì˜ˆì‹œ")

        target_doc = doc_ids[0]
        target_vec = st.session_state.doc_vectors[target_doc]
        other_vectors = [vec for i, vec in enumerate(vectors) if doc_ids[i] != target_doc]
        other_ids = [doc_ids[i] for i in range(len(doc_ids)) if doc_ids[i] != target_doc]

        top_idxs, _ = recommender.recommend_by_cosine(target_vec, other_vectors)
        top_docs = [other_ids[i] for i in top_idxs]
        pairs = [(doc_id, st.session_state.doc_texts[doc_id]) for doc_id in top_docs]
        explanation_json = recommender.explain_document_similarity(st.session_state.doc_texts[target_doc], pairs)

        try:
            explanation = json.loads(explanation_json)
            for rec in explanation["recommendations"]:
                st.markdown(f"ğŸ”— **{rec['document_id']}**: {rec['reason']}")
        except:
            st.error("ì¶”ì²œ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜")
